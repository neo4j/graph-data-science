[[algorithms-ml-nodeclassification]]
= Node Classification
:entity: node
:result: predicted property
:algorithm: Node Classification
:modelType: Node Classification


[abstract]
--
This section describes the Node Classification Model in the Neo4j Graph Data Science library.
--


[[algorithms-ml-nodeclassification-intro]]
== Introduction

Node Classification is a common machine learning task applied to graphs: training a model to learn in which class a node belongs.
Neo4j GDS trains supervised machine learning models based on node properties (features) in your graph to predict what class an unseen or future node would belong to.
Node Classification can be used favorably together with <<algorithms-ml-models-preprocessing, pre-processing algorithms>>.

Concretely, Node Classification models are used to predict a non-existing node property based on other node properties.
The non-existing node property represents the class, and is referred to as the target property.
The specified node properties are used as input features.
The Node Classification model does not rely on relationship information.
However, a node embedding algorithm could embed the neighborhoods of nodes as a node property, to transfer this information into the Node Classification model (see <<algorithms-node-embeddings>>).

Models are trained on parts of the input graph and evaluated using specified metrics.
Splitting of the graph into a train and a test graph is performed internally by the algorithm, and the test graph is used to evaluate model performance.

The training process follows this outline:

. The input graph is split into two parts: the train graph and the test graph.
. The train graph is further divided into a number of validation folds, each consisting of a train part and a validation part.
. Each model candidate is trained on each train part and evaluated on the respective validation part.
. The training process uses a logistic regression algorithm, and the evaluation uses the specified metrics.
  The first metric is the primary metric.
. The model with the highest average score according to the primary metric will win the training.
. The winning model will then be retrained on the entire train graph.
. The winning model is evaluated on the train graph as well as the test graph.
. The winning model is retrained on the entire original graph.
. Finally, the winning model will be registered in the <<model-catalog-ops, Model Catalog>>.

Trained models may then be used to predict the value of the `target` property (class) of previously unseen nodes.
In addition to the predicted class for each node, the predicted probability for each class may also be retained on the nodes.
The order of the probabilities match the order of the classes registered in the model.


[[algorithms-ml-nodeclassification-metrics]]
=== Metrics

The Node Classification model in the Neo4j GDS library supports the following evaluation metrics:

* Global metrics
** `F1_WEIGHTED`
** `F1_MACRO`
** `ACCURACY`

* Per-class metrics
** `F1(class=<number>)` or `F1(class=*)`
** `PRECISION(class=<number>)` or `PRECISION(class=*)`
** `RECALL(class=<number>)` or `RECALL(class=*)`
** `ACCURACY(class=<number>)` or `ACCURACY(class=*)`

The `*` is syntactic sugar for reporting the metric for each class in the graph.
When using a per-class metric, the reported metrics contain keys like for example `ACCURACY_class_1`.

More than one metric can be specified during training but only the first specified -- the `primary` one -- is used for evaluation, the results of all are present in the train results.
The primary metric may not be a `*` expansion due to the ambiguity of which of the expanded metrics should be the `primary` one.

[[algorithms-ml-nodeclassification-syntax]]
== Syntax

include::../../shared/syntax-intro-named-graph.adoc[]

.Node Classification syntax per mode
[.tabbed-example]
====

[.include-with-train]
======
.Run Node Classification in train mode on a named graph:
[source]
----
CALL gds.alpha.ml.nodeClassification.train(
  graphName: String,
  configuration: Map
) YIELD
  trainMillis: Integer,
  modelInfo: Map,
  configuration: Map
----

include::../../common-configuration/common-parameters-named-graph.adoc[]

include::../../common-configuration/common-train-configuration-named-graph.adoc[]

include::specific-train-configuration.adoc[]

.Results
[opts="header",cols="1,1,6"]
|===
| Name          | Type    | Description
| trainMillis   | Integer | Milliseconds used for training.
| modelInfo     | Map     | Information about the training and the winning model.
| configuration | Map     | Configuration used for the train procedure.
|===

The `modelInfo` can also be retrieved at a later time by using the <<catalog-model-list, Model List Procedure>>.
The `modelInfo` return field has the following algorithm-specific subfields:

[opts="header",cols="1,1,6"]
|===
| Name           | Type          | Description
| classes        | List of Integer | Sorted list of class ids which are the distinct values of `targetProperty` over the entire graph.
| bestParameters | Map           | The model parameters which performed best on average on validation folds according to the primary metric.
| metrics        | Map           | Map from metric description to evaluated metrics for various models and subsets of the data, see below.
|===

The `metrics` map contains for each metric description (such as `F1_MACRO` or `RECALL(class=4)`), the corresponding results for that metric.
Each such result map contains the keys `train`, `validation`, `outerTrain` and `test`.
The latter two correspond to numeric values for the evaluation of the best model on the test set and its complement -- the outer train set.
The `train` and `validation` results contain lists of results for all candidate models (i.e. `params`). Each such result is in turn also a map with keys `params`, `avg`, `min` and `max`.
These correspond to the hyper-parameters of the model candidate, and statistics for the result of that model over cross-validation folds.
The structure of `modelInfo` is:

[source]
----
{
    bestParameters: Map,     // one of the maps specified in `params`
    classes: List of Integer,
    metrics: {
        String: {            // a metric description
            "test": Float,
            "outerTrain": Float,
            "train": [{
                "avg": Float,
                "max": Float,
                "min": Float,
                "params": Map
            },
            {
                "avg": Float,
                "max": Float,
                "min": Float,
                "params": Map
            },
            ...              // more results per model candidate
            ],
            "validation": [{
                "avg": Float,
                "max": Float,
                "min": Float,
                "params": Map
            },
            {
                "avg": Float,
                "max": Float,
                "min": Float,
                "params": Map
            },
            ...              // more results per model candidate
            ],
        },
        String: Map,         // another metric
        ...                  // remaining metrics
    }
}
----
======

[.include-with-stream]
======
.Run Node Classification in stream mode on a named graph:
[source]
----
CALL gds.alpha.ml.nodeClassification.predict.stream(
  graphName: String,
  configuration: Map
) YIELD
  nodeId: Integer,
  predictedClass: Integer,
  predictedProbabilities: List[Float]
----

include::../../common-configuration/common-parameters-named-graph.adoc[]

include::../../common-configuration/common-stream-stats-configuration-named-graph.adoc[]

include::specific-stream-configuration.adoc[]

.Results
[opts="header",cols="1,1,6"]
|===
| Name                   | Type        | Description
| nodeId                 | Integer     | Node ID.
| predictedClass         | Integer     | Predicted class for this node.
| predictedProbabilities | List[Float] | Probabilities for all classes, for this node.
|===
======

[.include-with-mutate]
======
.Run Node Classification in mutate mode on a named graph:
[source]
----
CALL gds.alpha.ml.nodeClassification.predict.mutate(
  graphName: String,
  configuration: Map
) YIELD
  createMillis: Integer,
  computeMillis: Integer,
  postProcessingMillis: Integer,
  mutateMillis: Integer,
  nodePropertiesWritten: Integer,
  configuration: Map
----

include::../../common-configuration/common-parameters-named-graph.adoc[]

include::../../common-configuration/common-mutate-configuration-named-graph.adoc[]

include::specific-write-mutate-configuration.adoc[]

.Results
[opts="header",cols="1,1,6"]
|===
| Name                  | Type    | Description
| createMillis          | Integer | Milliseconds for creating the graph.
| computeMillis         | Integer | Milliseconds for running the algorithm.
| postProcessingMillis  | Integer | Milliseconds for computing the global metrics.
| mutateMillis          | Integer | Milliseconds for adding properties to the in-memory graph.
| nodePropertiesWritten | Integer | Number of relationships created.
| configuration         | Map     | Configuration used for running the algorithm.
|===
======

[.include-with-write]
======
.Run Node Classification in write mode on a named graph:
[source]
----
CALL gds.alpha.ml.nodeClassification.predict.write(
  graphName: String,
  configuration: Map
) YIELD
  createMillis: Integer,
  computeMillis: Integer,
  postProcessingMillis: Integer,
  writeMillis: Integer,
  nodePropertiesWritten: Integer,
  configuration: Map
----

include::../../common-configuration/common-parameters-named-graph.adoc[]

include::../../common-configuration/common-write-configuration-named-graph.adoc[]

include::specific-write-mutate-configuration.adoc[]

.Results
[opts="header",cols="1,1,6"]
|===
| Name                  | Type    | Description
| createMillis          | Integer | Milliseconds for creating the graph.
| computeMillis         | Integer | Milliseconds for running the algorithm.
| postProcessingMillis  | Integer | Milliseconds for computing the global metrics.
| writeMillis           | Integer | Milliseconds for writing result back to Neo4j.
| nodePropertiesWritten | Integer | Number of relationships created.
| configuration         | Map     | Configuration used for running the algorithm.
|===
======
====


[[algorithms-ml-nodeclassification-examples]]
== Examples

In this section we will show examples of training a Node Classification Model on a concrete graph.
The intention is to illustrate what the results look like and to provide a guide in how to make use of the model in a real setting.
We will do this on a small graph of a handful of nodes representing houses.
The example graph looks like this:

image::example-graphs/node_classification.svg[align="center"]

.The following Cypher statement will create the example graph in the Neo4j database:
[source, cypher, role=noplay setup-query]
----
CREATE
  (:House {color: 'Gold', sizePerStory: [15.5, 23.6, 33.1], class: 0}),
  (:House {color: 'Red', sizePerStory: [15.5, 23.6, 100.0], class: 0}),
  (:House {color: 'Blue', sizePerStory: [11.3, 35.1, 22.0], class: 0}),
  (:House {color: 'Green', sizePerStory: [23.2, 55.1, 0.0], class: 1}),
  (:House {color: 'Gray', sizePerStory: [34.3, 24.0, 0.0],  class: 1}),
  (:House {color: 'Black', sizePerStory: [71.66, 55.0, 0.0], class: 1}),
  (:House {color: 'White', sizePerStory: [11.1, 111.0, 0.0], class: 1}),
  (:House {color: 'Teal', sizePerStory: [80.8, 0.0, 0.0], class: 2}),
  (:House {color: 'Beige', sizePerStory: [106.2, 0.0, 0.0], class: 2}),
  (:House {color: 'Magenta', sizePerStory: [99.9, 0.0, 0.0], class: 2}),
  (:House {color: 'Purple', sizePerStory: [56.5, 0.0, 0.0], class: 2}),
  (:UnknownHouse {color: 'Pink', sizePerStory: [23.2, 55.1, 56.1]}),
  (:UnknownHouse {color: 'Tan', sizePerStory: [22.32, 102.0, 0.0]}),
  (:UnknownHouse {color: 'Yellow', sizePerStory: [39.0, 0.0, 0.0]});
----

With the graph in Neo4j we can now project it into the graph catalog to prepare it for algorithm execution.
We do this using a native projection targeting the `House` and `UnknownHouse` labels.
We will also project the `sizeOfStory` property to use as a model feature, and the `class` property to use as a target feature.

include::../../shared/examples-named-native-note.adoc[]

.The following statement will create a graph using a native projection and store it in the graph catalog under the name 'myGraph'.
[source, cypher, role=noplay graph-create-query]
----
CALL gds.graph.create('myGraph', {
    House: { properties: ['sizePerStory', 'class'] },
    UnknownHouse: { properties: 'sizePerStory' }
  },
  '*'
)
----

In the following examples we will demonstrate using the Node Classification model on this graph.


[[algorithms-ml-nodeclassification-examples-memory-estimation]]
=== Memory Estimation
:mode: train

include::../../shared/examples-estimate-intro.adoc[]

[role=query-example]
--
.The following will estimate the memory requirements for running the algorithm in write mode:
[source, cypher, role=noplay]
----
CALL gds.alpha.ml.nodeClassification.train.estimate('myGraph', {
  nodeLabels: ['House'],
  modelName: 'nc-model',
  featureProperties: ['sizePerStory'],
  targetProperty: 'class',
  randomSeed: 2,
  holdoutFraction: 0.2,
  validationFolds: 5,
  metrics: [ 'F1_WEIGHTED' ],
  params: [
    {penalty: 0.0625},
    {penalty: 0.5},
    {penalty: 1.0},
    {penalty: 4.0}
  ]
})
YIELD bytesMin, bytesMax, requiredMemory
----

.Results
[opts="header"]
|===
| bytesMin  | bytesMax  | requiredMemory
| 130874816 | 130906776 | +"[124 MiB ... 124 MiB]"+
|===
--


[[algorithms-ml-nodeclassification-examples-train]]
=== Train

In this example we will train a model to predict the class in which a house belongs, based on its `sizePerStory` property.

[[algorithms-ml-nodeclassification-examples-train-query]]
.Train a Node Classification model:
[source, cypher, role=noplay graph-create-query]
----
CALL gds.alpha.ml.nodeClassification.train('myGraph', {
  nodeLabels: ['House'],
  modelName: 'nc-model',
  featureProperties: ['sizePerStory'],
  targetProperty: 'class',
  randomSeed: 2,
  holdoutFraction: 0.2,
  validationFolds: 5,
  metrics: [ 'F1_WEIGHTED' ],
  params: [
    {penalty: 0.0625},
    {penalty: 0.5},
    {penalty: 1.0},
    {penalty: 4.0}
  ]
}) YIELD modelInfo
RETURN
  {penalty: modelInfo.bestParameters.penalty} AS winningModel,
  modelInfo.metrics.F1_WEIGHTED.outerTrain AS trainGraphScore,
  modelInfo.metrics.F1_WEIGHTED.test AS testGraphScore
----

.Results
[opts="header"]
|===
| winningModel     | trainGraphScore   | testGraphScore
| {penalty=0.0625} | 0.999999990909091 | 0.6363636286363638
|===

Here we can observe that the model candidate with penalty `0.0625` performed the best in the training phase, with a score of almost 100% over the train graph.
On the test graph, the model scores a bit lower at about 64%.
This indicates that the model reacted very well to the train graph, and was able to generalize fairly well to unseen data.
In order to achieve a higher test score, we may need to use better features, a larger graph, or different model configuration.


[[algorithms-ml-nodeclassification-examples-stream]]
=== Stream

include::../../shared/examples-stream-intro.adoc[]

In this example we will show how to use a trained model to predict the class of a node in your in-memory graph.
In addition to the predicted class, we will also produce the probability for each class in another node property.
In order to do this, we must first have an already trained model registered in the Model Catalog.
We will use the model which we trained in the <<algorithms-ml-nodeclassification-examples-train-query, train example>> which we gave the name `'nc-model'`.

[role=query-example, no-result=true, group=stream]
--
[source, cypher, role=noplay]
----
CALL gds.alpha.ml.nodeClassification.predict.stream('myGraph', {
  nodeLabels: ['House', 'UnknownHouse'],
  modelName: 'nc-model',
  includePredictedProbabilities: true
}) YIELD nodeId, predictedClass, predictedProbabilities
WITH gds.util.asNode(nodeId) AS houseNode, predictedClass, predictedProbabilities
WHERE houseNode:UnknownHouse
RETURN
  houseNode.color AS classifiedHouse,
  predictedClass,
  floor(predictedProbabilities[predictedClass] * 100) AS confidence
  ORDER BY classifiedHouse
----

.Results
[opts="header",cols="m,m,m"]
|===
| classifiedHouse | predictedClass | confidence
| "Pink"          |              0 | 98.0
| "Tan"           |              1 | 98.0
| "Yellow"        |              2 | 79.0
|===
--

As we can see, the model was able to predict the pink house into class 0, tan house into class 1, and yellow house into class 2.
This makes sense, as all houses in class 0 had three stories, class 1 two stories and class 2 one story, and the same is true of the pink, tan and yellow houses, respectively.
Additionally, we see that the model is confident in these predictions, as the highest class probability is >=80% in all cases.


[[algorithms-ml-nodeclassification-examples-mutate]]
=== Mutate

The `mutate` execution mode updates the named graph with a new node property containing the predicted class for that node.
The name of the new property is specified using the mandatory configuration parameter `mutateProperty`.
The result is a single summary row including information about timings and how many properties were written.
The `mutate` mode is especially useful when multiple algorithms are used in conjunction.

For more details on the `mutate` mode in general, see <<running-algos-mutate>>.

In this example we will show how to use a trained model to predict the class of a node in your in-memory graph.
In addition to the predicted class, we will also produce the probability for each class in another node property.
In order to do this, we must first have an already trained model registered in the Model Catalog.
We will use the model which we trained in the <<algorithms-ml-nodeclassification-examples-train-query, train example>> which we gave the name `'nc-model'`.

[role=query-example, group=mutate]
--
[source, cypher, role=noplay]
----
CALL gds.alpha.ml.nodeClassification.predict.mutate('myGraph', {
  nodeLabels: ['House', 'UnknownHouse'],
  modelName: 'nc-model',
  mutateProperty: 'predictedClass',
  predictedProbabilityProperty: 'predictedProbabilities'
}) YIELD nodePropertiesWritten
----

.Results
[opts="header"]
|===
| nodePropertiesWritten
| 28
|===
--

Since we specified also the `predictedProbabilityProperty` we are writing two properties for each of the 14 nodes.
In order to analyse our predicted classes we stream the properties from the in-memory graph:

[role=query-example, no-result=true, group=mutate]
--
[source, cypher, role=noplay]
----
CALL gds.graph.streamNodeProperties(
  'myGraph', ['predictedProbabilities', 'predictedClass'], ['UnknownHouse']
) YIELD nodeId, nodeProperty, propertyValue
RETURN gds.util.asNode(nodeId).color AS classifiedHouse, nodeProperty, propertyValue
  ORDER BY classifiedHouse, nodeProperty
----

.Results
[opts="header"]
|===
| classifiedHouse | nodeProperty             | propertyValue
| "Pink"          | "predictedClass"         | 0
| "Pink"          | "predictedProbabilities" | [0.9866455686217779, 0.01311656378786989, 2.3786759035214687E-4]
| "Tan"           | "predictedClass"         | 1
| "Tan"           | "predictedProbabilities" | [0.01749164563726576, 0.9824922482993587, 1.610606337562594E-5]
| "Yellow"        | "predictedClass"         | 2
| "Yellow"        | "predictedProbabilities" | [0.0385634113659007, 0.16350471177895198, 0.7979318768551473]
|===
--

As we can see, the model was able to predict the pink house into class 0, tan house into class 1, and yellow house into class 2.
This makes sense, as all houses in class 0 had three stories, class 1 two stories and class 2 one story, and the same is true of the pink, tan and yellow houses, respectively.
Additionally, we see that the model is confident in these predictions, as the highest class probability is >75% in all cases.


[[algorithms-ml-nodeclassification-examples-write]]
=== Write

The `write` execution mode writes the predicted property for each node as a property to the Neo4j database.
The name of the new property is specified using the mandatory configuration parameter `writeProperty`.
The result is a single summary row including information about timings and how many properties were written.
The `write` mode enables directly persisting the results to the database.

For more details on the `write` mode in general, see <<running-algos-write>>.

In this example we will show how to use a trained model to predict the class of a node in your in-memory graph.
In addition to the predicted class, we will also produce the probability for each class in another node property.
In order to do this, we must first have an already trained model registered in the Model Catalog.
We will use the model which we trained in the <<algorithms-ml-nodeclassification-examples-train-query, train example>> which we gave the name `'nc-model'`.

[role=query-example, group=write]
--
[source, cypher, role=noplay]
----
CALL gds.alpha.ml.nodeClassification.predict.write('myGraph', {
  nodeLabels: ['House', 'UnknownHouse'],
  modelName: 'nc-model',
  writeProperty: 'predictedClass',
  predictedProbabilityProperty: 'predictedProbabilities'
}) YIELD nodePropertiesWritten
----

.Results
[opts="header"]
|===
| nodePropertiesWritten
| 28
|===
--

Since we specified also the `predictedProbabilityProperty` we are writing two properties for each of the 14 nodes.
In order to analyse our predicted classes we stream the properties from the in-memory graph:

[role=query-example, no-result=true, group=write]
--
[source, cypher, role=noplay]
----
MATCH (house:UnknownHouse)
RETURN house.color AS classifiedHouse, house.predictedClass AS predictedClass, house.predictedProbabilities AS predictedProbabilities
----

.Results
[opts="header"]
|===
| classifiedHouse | predictedClass | predictedProbabilities
| "Pink"          |              0 | [0.9866455686217779, 0.01311656378786989, 2.3786759035214687E-4]
| "Tan"           |              1 | [0.01749164563726576, 0.9824922482993587, 1.610606337562594E-5]
| "Yellow"        |              2 | [0.0385634113659007, 0.16350471177895198, 0.7979318768551473]
|===
--


As we can see, the model was able to predict the pink house into class 0, tan house into class 1, and yellow house into class 2.
This makes sense, as all houses in class 0 had three stories, class 1 two stories and class 2 one story, and the same is true of the pink, tan and yellow houses, respectively.
Additionally, we see that the model is confident in these predictions, as the highest class probability is >75% in all cases.
